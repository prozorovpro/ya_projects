# Определение токсичных комментариев

---

Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. 


## Описание набора данных

В нашем распоряжении набор данных с разметкой о токсичности правок.
Столбец text в нём содержит текст комментария, а toxic — целевой признак.

## Инструменты

Укажите используемые инструменты и библиотеки, которые были применены в проекте:

- pandas
- numpy
- matplotlib
- sklearn
- catboost
- nltk
- предобученая модель [Bert](https://huggingface.co/unitary/toxic-bert)

## Основные шаги исследования


1. Изучение датасета
2. Подготовка данных
    - Лематизация 
    - Удаление лишних символов, дубликатов
    - Создание новых признаков
    - Разделение на выборки
3. Обучение классических моделей
4. Получение предсказаний с предобученой модели [Bert](https://huggingface.co/unitary/toxic-bert)

## Выводы

В ходе работы проведена подготовка данных для рассмотрения в разных моделях
Были созданы фичи с эмоциональной окраской текста, длиной текста, количеством нецензурных слов
Были рассмотрены модели:

Модель логистической регресии
CatBoost c использованием встроеной функции для построения модели на текстовых данных, и без неё
CatBoost только на фичах полученых в результате предобработки данных Так же отдельно для предсказания была использована уже предобученая модель Detoxify
Самый лучшей результат среди моделей обученых на предоставленых данных оказалась модель градиентного бустинга обученая на сгенерированых фичах и без использования встроеной функции обработки текста
результат работы модели на тестовых данных: F1 0,78

Результат работы модели Detoxify, предобученой для определения тональности текста, на всех данных F1: 0.94

## Ссылки

[Ноутбук](https://github.com/prozorovpro/ya_projects/blob/main/%D0%9E%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%82%D0%BE%D0%BA%D1%81%D0%B8%D1%87%D0%BD%D1%8B%D1%85%20%D0%BA%D0%BE%D0%BC%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D1%80%D0%B8%D0%B5%D0%B2/toxic_comment_classification.ipynb)
[Colab c BERT](https://colab.research.google.com/drive/10tynAw3NoKEEN5Q9VKTIImIq_BpQkpS6?usp=sharing#scrollTo=FihagrsXiam2)